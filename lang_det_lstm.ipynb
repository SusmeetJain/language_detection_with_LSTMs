{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reading this notebook:\n",
    "This notebook does not use a lot of markdowns, but is well commented, reading through the comments shall get you thorough the code!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Lets extract our sentences and have a look at the data we will be dealing with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>language</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>69347</td>\n",
       "      <td>69347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>69347</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>Ernesto estió o primero en prener o títol.</td>\n",
       "      <td>Galego</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>1</td>\n",
       "      <td>10000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          sentence language\n",
       "count                                        69347    69347\n",
       "unique                                       69347        7\n",
       "top     Ernesto estió o primero en prener o títol.   Galego\n",
       "freq                                             1    10000"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv(\"all-sentences.txt\", names=[\"sentence\", \"language\"], header=None, delimiter=\"|\")\n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We have got a total of 69,347 different sentences in 7 unique languages, our task is to train an Reccurent neural network based on LSTMs to correctly identify which language a sentence is written in.\n",
    "- At first this seems like a trivial task but what's important here is we are not defining any rules to identify the language, instead just giving the data to an neural network to train on for it to correcrtly identify the language!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>language</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Cuando llegaron los manifestantes, la escuela ...</td>\n",
       "      <td>Castellano</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Experto Comisión Mundial de Áreas Protegidas –...</td>\n",
       "      <td>Castellano</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ya habían pasado tres años de la condena y seg...</td>\n",
       "      <td>Castellano</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>“Buscamos la adhesión porque todos tenemos una...</td>\n",
       "      <td>Castellano</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>La aplicación del Plan con los alumnos se real...</td>\n",
       "      <td>Castellano</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sentence    language\n",
       "0  Cuando llegaron los manifestantes, la escuela ...  Castellano\n",
       "1  Experto Comisión Mundial de Áreas Protegidas –...  Castellano\n",
       "2  Ya habían pasado tres años de la condena y seg...  Castellano\n",
       "3  “Buscamos la adhesión porque todos tenemos una...  Castellano\n",
       "4  La aplicación del Plan con los alumnos se real...  Castellano"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is how the data is organised"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Cleaning and re structuring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def process_sentence(sentence):\n",
    "    '''Removes all special characters from sentence. It will also strip out\n",
    "    extra whitespace and makes the string lowercase.\n",
    "    '''\n",
    "    return re.sub(r'[\\\\\\\\/:*«`\\'?¿\";!<>,.|]', '', sentence.lower().strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedShuffleSplit, train_test_split\n",
    "\n",
    "# As our sentences in all_sentences.txt are in order, we need to shuffle it first.\n",
    "sss = StratifiedShuffleSplit(test_size=0.2, random_state=0)\n",
    "\n",
    "# Clean the sentences\n",
    "X = data[\"sentence\"].apply(process_sentence)\n",
    "y = data[\"language\"]\n",
    "\n",
    "# Split all our sentences\n",
    "elements = (' '.join([sentence for sentence in X])).split()\n",
    "\n",
    "X_train, X_test, y_train, y_test = None, None, None, None\n",
    "\n",
    "for train_index, test_index in sss.split(X, y):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have split our data into training and testing sets, our target shall be to acheive as good a accuracy as possible on the test data, the data our neural network hasn't seen while training!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### basic EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Languages in our dataset: {'Català', 'Euskara', 'Aragonés', 'Castellano', 'Asturianu', 'Occitan', 'Galego'}\n"
     ]
    }
   ],
   "source": [
    "languages = set(y)\n",
    "print(\"Languages in our dataset: {}\".format(languages))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Shapes:\n",
      "\tTrain set: \t\t(55477,) \n",
      "\tTest set: \t\t(13870,)\n",
      "Totals:\n",
      "\tWords in our Dataset: 1348815\n",
      "\tLanguages: 7\n"
     ]
    }
   ],
   "source": [
    "print(\"Feature Shapes:\")\n",
    "print(\"\\tTrain set: \\t\\t{}\".format(X_train.shape),\n",
    "      \"\\n\\tTest set: \\t\\t{}\".format(X_test.shape))\n",
    "print(\"Totals:\\n\\tWords in our Dataset: {}\\n\\tLanguages: {}\".format(len(elements), len(languages)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pre Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to convert our text data into a suitable format for the neural networks to consume (numbers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_lookup_tables(text):\n",
    "    \"\"\"Create lookup tables for vocabulary\n",
    "    :param text: The text split into words\n",
    "    :return: A tuple of dicts (vocab_to_int, int_to_vocab)\n",
    "    \"\"\"\n",
    "    vocab = set(text)\n",
    "    \n",
    "    vocab_to_int = {word: i for i, word in enumerate(vocab)}\n",
    "    int_to_vocab = {v:k for k, v in vocab_to_int.items()}\n",
    "    \n",
    "    return vocab_to_int, int_to_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary of our dataset: 187734\n"
     ]
    }
   ],
   "source": [
    "elements.append(\"<UNK>\")\n",
    "\n",
    "# Map our vocabulary to int\n",
    "vocab_to_int, int_to_vocab = create_lookup_tables(elements)\n",
    "languages_to_int, int_to_languages = create_lookup_tables(y)\n",
    "\n",
    "print(\"Vocabulary of our dataset: {}\".format(len(vocab_to_int)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_int(data, data_int):\n",
    "    \"\"\"Converts all our text to integers\n",
    "    :param data: The text to be converted\n",
    "    :return: All sentences in ints\n",
    "    \"\"\"\n",
    "    all_items = []\n",
    "    for sentence in data: \n",
    "        all_items.append([data_int[word] if word in data_int else data_int[\"<UNK>\"] for word in sentence.split()])\n",
    "    \n",
    "    return all_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert our inputs\n",
    "X_test_encoded = convert_to_int(X_test, vocab_to_int)\n",
    "X_train_encoded = convert_to_int(X_train, vocab_to_int)\n",
    "\n",
    "y_data = convert_to_int(y_test, languages_to_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "enc = OneHotEncoder()\n",
    "\n",
    "enc.fit(y_data)\n",
    "\n",
    "# One hot encoding our outputs\n",
    "y_train_encoded = enc.fit_transform(convert_to_int(y_train, languages_to_int)).toarray()\n",
    "y_test_encoded = enc.fit_transform(convert_to_int(y_test, languages_to_int)).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Making sure the system configurations are right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\deep_learning\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow Version: 1.7.0\n",
      "Default GPU Device: /device:GPU:0\n"
     ]
    }
   ],
   "source": [
    "from distutils.version import LooseVersion\n",
    "import warnings\n",
    "import tensorflow as tf\n",
    "\n",
    "# Check TensorFlow Version\n",
    "assert LooseVersion(tf.__version__) >= LooseVersion('1.0'), 'Please use TensorFlow version 1.0 or newer'\n",
    "print('TensorFlow Version: {}'.format(tf.__version__))\n",
    "\n",
    "# Check for a GPU\n",
    "if not tf.test.gpu_device_name():\n",
    "    warnings.warn('No GPU found. Please use a GPU to train your neural network.')\n",
    "else:\n",
    "    print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "max_sentence_length = 200\n",
    "embedding_vector_length = 300\n",
    "dropout = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Import Keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Truncate and pad input sentences\n",
    "X_train_pad = sequence.pad_sequences(X_train_encoded, maxlen=max_sentence_length)\n",
    "X_test_pad = sequence.pad_sequences(X_test_encoded, maxlen=max_sentence_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 200, 300)          56320200  \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 200, 256)          570368    \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 256)               525312    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 7)                 1799      \n",
      "=================================================================\n",
      "Total params: 57,417,679\n",
      "Trainable params: 57,417,679\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Create the model\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Embedding(len(vocab_to_int), embedding_vector_length, input_length=max_sentence_length))\n",
    "model.add(LSTM(256, return_sequences=True, dropout=dropout, recurrent_dropout=dropout))\n",
    "model.add(LSTM(256, dropout=dropout, recurrent_dropout=dropout))\n",
    "model.add(Dense(len(languages), activation='softmax'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 200, 300)          56320200  \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 200, 256)          570368    \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 256)               525312    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 7)                 1799      \n",
      "=================================================================\n",
      "Total params: 57,417,679\n",
      "Trainable params: 57,417,679\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "55477/55477 [==============================] - 143s 3ms/step - loss: 0.1112 - acc: 0.9586\n",
      "Epoch 2/2\n",
      "55477/55477 [==============================] - 140s 3ms/step - loss: 0.0108 - acc: 0.9967\n",
      "Accuracy: 99.64%\n"
     ]
    }
   ],
   "source": [
    "# Train the model# Train  \n",
    "model.fit(X_train_pad, y_train_encoded, epochs=2, batch_size=256)\n",
    "\n",
    "# Final evaluation of the model\n",
    "scores = model.evaluate(X_test_pad, y_test_encoded, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conclusions:\n",
    "A simple 2 layer LSTM trained for only 2 epochs gives a commendable accuracy pf 99.64% on unseen data in identifying the correct language of a given sentence. This captures how powerful RNNs are.\n",
    "\n",
    "#### Next Steps:\n",
    "The next step in this project is to take on the problem on a more challenging dataset, the Europarl dataset looks like a good one to pick!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
